import requests
from bs4 import BeautifulSoup
import json
import time
import random
import re
import os
import datetime
from tqdm import tqdm
from dotenv import load_dotenv
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted, InvalidArgument

# ==========================================
# [ì„¤ì •]
# ==========================================
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GOOGLE_MAPS_API_KEY = os.getenv("GOOGLE_MAPS_API_KEY")

if not GEMINI_API_KEY or not GOOGLE_MAPS_API_KEY:
    print("âŒ ì˜¤ë¥˜: .env íŒŒì¼ì— API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

genai.configure(api_key=GEMINI_API_KEY)

MODEL_NAME = 'gemini-2.0-flash'
try:
    model = genai.GenerativeModel(MODEL_NAME)
    print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {MODEL_NAME}")
except Exception as e:
    print(f"âš ï¸ ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")
    model = genai.GenerativeModel('gemini-pro')

OUTPUT_JSON = "file/schools_complete_db.json"

# ì¼ë³¸ ì „êµ­ 47ê°œ ë„ë„ë¶€í˜„ ë¦¬ìŠ¤íŠ¸
PREFECTURES = [
    "åŒ—æµ·é“", "é’æ£®", "å²©æ‰‹", "å®®åŸ", "ç§‹ç”°", "å±±å½¢", "ç¦å³¶",
    "èŒ¨åŸ", "æ ƒæœ¨", "ç¾¤é¦¬", "åŸ¼ç‰", "åƒè‘‰", "æ±äº¬éƒ½", "ç¥å¥ˆå·",
    "æ–°æ½Ÿ", "å¯Œå±±", "çŸ³å·", "ç¦äº•", "å±±æ¢¨", "é•·é‡", "å²é˜œ", "é™å²¡", "æ„›çŸ¥",
    "ä¸‰é‡", "æ»‹è³€", "äº¬éƒ½", "å¤§é˜ª", "å…µåº«", "å¥ˆè‰¯", "å’Œæ­Œå±±",
    "é³¥å–", "å³¶æ ¹", "å²¡å±±", "åºƒå³¶", "å±±å£",
    "å¾³å³¶", "é¦™å·", "æ„›åª›", "é«˜çŸ¥",
    "ç¦å²¡", "ä½è³€", "é•·å´", "ç†Šæœ¬", "å¤§åˆ†", "å®®å´", "é¹¿å…å³¶", "æ²–ç¸„"
]

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

# ==========================================
# [í•¨ìˆ˜ ì •ì˜]
# ==========================================
def clean_address_string(address):
    if not address: return ""
    address = re.sub(r'ã€’?\s*\d{3}-\d{4}', '', address)
    address = address.strip()
    if ' ' in address:
        address = address.split(' ')[0]
    return address

def get_google_coordinates(address):
    base_url = "https://maps.googleapis.com/maps/api/geocode/json"
    clean_address = clean_address_string(address)
    if not clean_address: return None
    params = {"address": clean_address, "key": GOOGLE_MAPS_API_KEY, "language": "ja"}
    try:
        res = requests.get(base_url, params=params)
        data = res.json()
        if data['status'] == 'OK':
            loc = data['results'][0]['geometry']['location']
            return {"lat": loc['lat'], "lng": loc['lng']}
    except: pass
    return None

def clean_json(text):
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    return text.strip()

# [ìˆ˜ì •] ì˜¤íƒ€ ìˆ˜ì •ë¨ (ddef -> def)
def extract_info_ai(school_name, text):
    prompt = f"""
    You are a data extractor. Extract info from the text into a strict JSON format.
    [Rules]
    1. Numbers: Remove commas, convert to Integer. If missing, use 0.
    2. 'features': Extract keywords based on the text. Mandatory: 'ê¸°ìˆ™ì‚¬', '1ì¸ì‹¤', 'ì¥í•™ê¸ˆ', 'EJU', 'ì´ê³¼', 'ì§„í•™', 'íšŒí™”', 'ë¹„ì¦ˆë‹ˆìŠ¤', 'ë¯¸ìˆ ', 'ë””ìì¸', 'ë‹¨ê¸°'.
    3. 'major_universities': Extract specific names of universities/grad schools. 
       **IMPORTANT**: Extract names in **Japanese Kanji** (e.g., æ—©ç¨²ç”°å¤§å­¦, æ±äº¬å¤§å­¦). Limit to 10 names.
    4. 'description_ko': Summarize the school's characteristics in Korean (2-3 sentences). 
       Focus on location, unique courses, or atmosphere. (e.g., "ë„ì¿„ ì‹ ì£¼ì¿ ì— ìœ„ì¹˜í•˜ë©° ì§„í•™ ì§€ë„ì— ê°•ì ì´ ìˆëŠ” í•™êµì…ë‹ˆë‹¤...")
    5. Output: ONLY valid JSON string.

    [Schema]
    {{
        "id": "unique_id_english_or_number",
        "basic_info": {{ "name_ja": "School Name", "address": "Address", "capacity": "Capacity(int)" }},
        "student_demographics": {{ "total": "Total(int)", "korea": "Korea(int)", "china": "China(int)", "vietnam": "Vietnam(int)", "nepal": "Nepal(int)", "usa": "USA(int)" }},
        "courses": [ {{ "course_name": "Name", "admission_month": "Month", "total_fees": "1st Year Fee(int)" }} ],
        "career_path": {{ "grad_school": "Grad(int)", "university": "Univ(int)", "vocational": "Vocational(int)", "major_universities": ["æ—©ç¨²ç”°å¤§å­¦", "æ˜æ²»å¤§å­¦"] }},
        "features": ["Feature1", "Feature2", "ê¸°ìˆ™ì‚¬", ...],
        "description_ko": "í•™êµ ì†Œê°œê¸€..."
    }}
    [Text] {school_name} \n {text[:15000]}
    """
    
    for i in range(3):
        try:
            res = model.generate_content(prompt)
            if not res.text: return None
            return json.loads(clean_json(res.text))
        except ResourceExhausted:
            time.sleep((i + 1) * 10)
        except Exception as e:
            print(f"   âŒ AI ì˜¤ë¥˜ ({school_name}): {e}")
            return None
    return None

def get_school_links(area_name):
    target_url = f"https://www.nisshinkyo.org/search/area.php?lng=1&area={area_name}"
    try:
        res = requests.get(target_url, headers=HEADERS)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        links = soup.select('a[href*="college.php"]')
        return [{"name": l.get_text(strip=True), "url": f"https://www.nisshinkyo.org/search/{l['href']}"} for l in links]
    except: return []

def get_page_text(url):
    try:
        time.sleep(random.uniform(0.5, 1.0))
        res = requests.get(url, headers=HEADERS)
        res.encoding = res.apparent_encoding
        if res.status_code != 200: return None
        soup = BeautifulSoup(res.text, 'html.parser')
        for s in soup(["script", "style"]): s.extract()
        return soup.get_text("\n", strip=True)
    except: return None

# ì‹¤ì‹œê°„ ì €ì¥ í•¨ìˆ˜
def save_db(data_list):
    structure = {
        "last_updated": datetime.date.today().strftime("%Y-%m-%d"),
        "schools": data_list
    }
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(structure, f, ensure_ascii=False, indent=2)

def load_existing_db():
    if os.path.exists(OUTPUT_JSON):
        try:
            with open(OUTPUT_JSON, 'r', encoding='utf-8') as f:
                content = f.read()
                if not content: return []
                data = json.loads(content)
                if isinstance(data, dict) and "schools" in data: return data["schools"]
                elif isinstance(data, list): return data
        except: pass
    return []

# ==========================================
# [ë©”ì¸ ì‹¤í–‰]
# ==========================================
def main():
    if not os.path.exists("file"): os.makedirs("file")
    
    # 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (ì´ì–´í•˜ê¸°)
    existing_data = load_existing_db()
    existing_urls = {s.get('source_url') for s in existing_data if s.get('source_url')}
    print(f"ğŸ“‚ ê¸°ì¡´ ë°ì´í„°: {len(existing_data)}ê°œ ë¡œë“œë¨ (ì—¬ê¸°ë¶€í„° ì´ì–´ì„œ ì§„í–‰)")
    
    # 2. ì „ì²´ ë§í¬ ìˆ˜ì§‘
    all_links = []
    print(f"ğŸ” ì¼ë³¸ ì „êµ­ {len(PREFECTURES)}ê°œ ë„ë„ë¶€í˜„ ê²€ìƒ‰ ì¤‘...")
    for pref in tqdm(PREFECTURES, desc="ì§€ì—­ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘"):
        links = get_school_links(pref)
        all_links.extend(links)
        time.sleep(0.1)
    
    # 3. ì•„ì§ ì•ˆ í•œ í•™êµë§Œ ê³¨ë¼ë‚´ê¸°
    new_targets = [s for s in all_links if s['url'] not in existing_urls]
    
    if not new_targets:
        print("âœ¨ ëª¨ë“  í•™êµê°€ ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        return

    print(f"ğŸš€ ë‚¨ì€ {len(new_targets)}ê°œ í•™êµ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 4. í•˜ë‚˜ì”© ì²˜ë¦¬í•˜ê³  ì¦‰ì‹œ ì €ì¥
    count = 0
    for school in tqdm(new_targets, desc="AI ì²˜ë¦¬ ë° ìë™ì €ì¥"):
        raw_text = get_page_text(school['url'])
        if not raw_text: continue
        
        data = extract_info_ai(school['name'], raw_text)
        
        if data:
            data['source_url'] = school['url']
            addr = data['basic_info'].get('address', '')
            if addr:
                coords = get_google_coordinates(addr)
                if coords: data['location'] = coords
            
            # [í•µì‹¬] ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ê³  ë°”ë¡œ íŒŒì¼ ì €ì¥
            existing_data.append(data)
            save_db(existing_data)
            
            count += 1
            time.sleep(4.5) # ë¬´ë£Œ í‹°ì–´ ì•ˆì „ ë”œë ˆì´

    print(f"\nğŸ‰ ìµœì¢… ì™„ë£Œ! ì´ {len(existing_data)}ê°œ ì €ì¥ë¨ (ì´ë²ˆì— {count}ê°œ ì¶”ê°€)")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ëŠ” ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")