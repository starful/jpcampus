import requests
from bs4 import BeautifulSoup
import json
import time
import random
import re
import os
import datetime
from tqdm import tqdm
from dotenv import load_dotenv
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted

# ==========================================
# [ì„¤ì •]
# ==========================================
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GOOGLE_MAPS_API_KEY = os.getenv("GOOGLE_MAPS_API_KEY")

if not GEMINI_API_KEY or not GOOGLE_MAPS_API_KEY:
    print("âŒ ì˜¤ë¥˜: .env íŒŒì¼ì— API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

genai.configure(api_key=GEMINI_API_KEY)

MODEL_NAME = 'gemini-2.0-flash'
try:
    model = genai.GenerativeModel(MODEL_NAME)
    print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {MODEL_NAME}")
except:
    print("âš ï¸ ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‹œë„")
    model = genai.GenerativeModel('gemini-pro')

OUTPUT_JSON = "file/schools_complete_db.json"

TARGET_AREAS = [
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E5%9F%BC%E7%8E%89",         # ì‚¬ì´íƒ€ë§ˆ
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E5%8D%83%E8%91%89",         # ì¹˜ë°”
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E6%9D%B1%E4%BA%AC%E9%83%BD", # ë„ì¿„
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E7%A5%9E%E5%A5%88%E5%B7%9D", # ê°€ë‚˜ê°€ì™€
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E4%BA%AC%E9%83%BD",         # êµí† 
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E5%A4%A7%E9%98%AA",         # ì˜¤ì‚¬ì¹´
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E5%85%B5%E5%BA%AB",         # íš¨ê³ 
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E7%A6%8F%E5%B2%A1",         # í›„ì¿ ì˜¤ì¹´
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E5%8C%97%E6%B5%B7%E9%81%93", # í™‹ì¹´ì´ë„
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E6%84%9B%E7%9F%A5"          # ì•„ì´ì¹˜
]

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

# ==========================================
# [1] Google Geocoding (fix_coords ê¸°ëŠ¥ í†µí•©)
# ==========================================
def clean_address_string(address):
    """ì£¼ì†Œ ì „ì²˜ë¦¬: ìš°í¸ë²ˆí˜¸ ì œê±°, ê³µë°± ë’¤ ê±´ë¬¼ëª… ì œê±°"""
    if not address: return ""
    # ìš°í¸ë²ˆí˜¸ ì œê±° (ã€’123-4567, 123-4567 ë“±)
    address = re.sub(r'ã€’?\s*\d{3}-\d{4}', '', address)
    # ì•ë’¤ ê³µë°± ì œê±°
    address = address.strip()
    # ê³µë°±ì´ ìˆë‹¤ë©´ ì•ë¶€ë¶„(ë²ˆì§€ìˆ˜)ë§Œ ê°€ì ¸ì˜¤ê¸° (ê±´ë¬¼ëª… ì œê±°)
    if ' ' in address:
        address = address.split(' ')[0]
    return address

def get_google_coordinates(address):
    base_url = "https://maps.googleapis.com/maps/api/geocode/json"
    clean_address = clean_address_string(address)
    
    if not clean_address:
        return None

    params = {"address": clean_address, "key": GOOGLE_MAPS_API_KEY, "language": "ja"}
    
    try:
        res = requests.get(base_url, params=params)
        data = res.json()
        if data['status'] == 'OK':
            loc = data['results'][0]['geometry']['location']
            return {"lat": loc['lat'], "lng": loc['lng']}
        else:
            print(f"   âš ï¸ ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨ [{data['status']}]: {clean_address}")
    except Exception as e:
        print(f"   âš ï¸ API ìš”ì²­ ì—ëŸ¬: {e}")
    
    return None

# ==========================================
# [2] í¬ë¡¤ë§ & AI ë³€í™˜
# ==========================================
def clean_json(text):
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    return text.strip()

def extract_info_ai(school_name, text):
    prompt = f"""
    Extract data to JSON.
    [Rules]
    1. Numbers: Remove commas, convert to Integer.
    2. Missing: Use 0 or null.
    3. Output: ONLY valid JSON string.

    [Schema]
    {{
        "id": "unique_id_english_or_number",
        "basic_info": {{ "name_ja": "School Name", "address": "Address", "capacity": "Capacity(int)" }},
        "student_demographics": {{ "total": "Total(int)", "korea": "Korea(int)", "china": "China(int)", "vietnam": "Vietnam(int)", "nepal": "Nepal(int)", "usa": "USA(int)" }},
        "courses": [ {{ "course_name": "Name", "admission_month": "Month", "total_fees": "1st Year Fee(int)" }} ],
        "career_path": {{ "grad_school": "Grad(int)", "university": "Univ(int)", "vocational": "Vocational(int)" }},
        "features": ["Feature1", "Feature2"]
    }}
    [Text] {school_name} \n {text[:15000]}
    """
    
    max_retries = 3
    for i in range(max_retries):
        try:
            res = model.generate_content(prompt)
            return json.loads(clean_json(res.text))
        except ResourceExhausted:
            wait_time = (i + 1) * 10
            print(f"   â³ API í•œë„ ì´ˆê³¼! {wait_time}ì´ˆ ëŒ€ê¸°...")
            time.sleep(wait_time)
        except Exception as e:
            print(f"   âš ï¸ ë³€í™˜ ì—ëŸ¬: {e}")
            return None
    return None

def get_school_links(target_url):
    try:
        res = requests.get(target_url, headers=HEADERS)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        links = soup.select('a[href*="college.php"]')
        return [{"name": l.get_text(strip=True), "url": f"https://www.nisshinkyo.org/search/{l['href']}"} for l in links]
    except Exception as e:
        print(f"ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨ ({target_url}): {e}")
        return []

def get_page_text(url):
    try:
        time.sleep(random.uniform(0.5, 1.0))
        res = requests.get(url, headers=HEADERS)
        res.encoding = res.apparent_encoding
        if res.status_code != 200: return None
        soup = BeautifulSoup(res.text, 'html.parser')
        for s in soup(["script", "style"]): s.extract()
        return soup.get_text("\n", strip=True)
    except:
        return None

def load_existing_db():
    """ê¸°ì¡´ DB íŒŒì¼ ë¡œë“œ"""
    if os.path.exists(OUTPUT_JSON):
        try:
            with open(OUTPUT_JSON, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # ì‹ ê·œ í¬ë§·(dict) vs êµ¬ë²„ì „ í¬ë§·(list) ì²˜ë¦¬
                if isinstance(data, dict) and "schools" in data:
                    return data["schools"]
                elif isinstance(data, list):
                    return data
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
    return []

# ==========================================
# [3] ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    if not os.path.exists("file"): os.makedirs("file")
    
    # 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    existing_data = load_existing_db()
    existing_urls = {s.get('source_url') for s in existing_data if s.get('source_url')}
    print(f"ğŸ“‚ ê¸°ì¡´ ë°ì´í„°: {len(existing_data)}ê°œ ë¡œë“œë¨")

    # 2. í¬ë¡¤ë§ í•  URL ìˆ˜ì§‘
    all_links = []
    print("ğŸ” ê° ì§€ì—­ë³„ í•™êµ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘...")
    for area_url in TARGET_AREAS:
        links = get_school_links(area_url)
        all_links.extend(links)
    
    # 3. ì‹ ê·œ í•™êµ í•„í„°ë§ (ì¤‘ë³µ ì œê±° ë° ê¸°ì¡´ DBì— ì—†ëŠ” ê²ƒë§Œ)
    seen_urls_in_crawl = set()
    new_targets = []
    
    for s in all_links:
        url = s['url']
        if url not in seen_urls_in_crawl and url not in existing_urls:
            new_targets.append(s)
            seen_urls_in_crawl.add(url)
    
    print(f"ğŸ“Š ê²€ìƒ‰ëœ ì „ì²´ í•™êµ: {len(seen_urls_in_crawl)}ê°œ")
    print(f"ğŸ†• ì¶”ê°€í•  ì‹ ê·œ í•™êµ: {len(new_targets)}ê°œ")
    
    if len(new_targets) == 0:
        print("âœ¨ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œë§Œ ê°±ì‹ í•©ë‹ˆë‹¤.")
    else:
        print("ğŸš€ ì‹ ê·œ í•™êµ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ (AI ë¶„ì„ + ì¢Œí‘œ ë³€í™˜)...")

    # 4. ì‹ ê·œ ë°ì´í„° ì²˜ë¦¬ Loop
    new_data_list = []
    for school in tqdm(new_targets):
        raw_text = get_page_text(school['url'])
        if not raw_text: continue

        # AI ë°ì´í„° ì¶”ì¶œ
        data = extract_info_ai(school['name'], raw_text)
        if data:
            data['source_url'] = school['url']
            
            # [í†µí•©] ì¢Œí‘œ ë³€í™˜ ì¦‰ì‹œ ì‹¤í–‰
            addr = data['basic_info'].get('address', '')
            if addr:
                coords = get_google_coordinates(addr)
                if coords:
                    data['location'] = coords
            
            new_data_list.append(data)
            time.sleep(1.5) # API ì†ë„ ì¡°ì ˆ

    # 5. ê¸°ì¡´ ë°ì´í„° + ì‹ ê·œ ë°ì´í„° ë³‘í•©
    final_list = existing_data + new_data_list
    
    # 6. ì €ì¥
    today_str = datetime.date.today().strftime("%Y-%m-%d")
    final_structure = {
        "last_updated": today_str,
        "schools": final_list
    }

    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(final_structure, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ ì™„ë£Œ! ì´ {len(final_list)}ê°œ ì €ì¥ë¨ (ì‹ ê·œ ì¶”ê°€: {len(new_data_list)}ê°œ)")
    print(f"ğŸ“ íŒŒì¼ ìœ„ì¹˜: {OUTPUT_JSON}")

if __name__ == "__main__":
    main()