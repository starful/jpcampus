import csv
import json
import os
import time
import requests
from tqdm import tqdm
from dotenv import load_dotenv
import google.generativeai as genai

# ==========================================
# [ì„¤ì •]
# ==========================================
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GOOGLE_MAPS_API_KEY = os.getenv("GOOGLE_MAPS_API_KEY")

# ğŸ¯ ìƒì„±í•  ëŒ€í•™ ê°œìˆ˜ ì„¤ì • (0ì´ë©´ ì œí•œ ì—†ìŒ)
LIMIT = 10

# ê²½ë¡œ ì„¤ì •
INPUT_CSV = "scripts/file/univ_list_100.csv"
OUTPUT_DIR = "app/content"
LOG_DIR = "logs"
HISTORY_FILE = os.path.join(LOG_DIR, "univ_processed_history.txt")

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-2.0-flash')

# ==========================================
# [í•¨ìˆ˜ ì •ì˜]
# ==========================================

def load_history():
    """ì²˜ë¦¬ëœ í•™êµ ëª©ë¡ ë¡œë“œ"""
    if not os.path.exists(HISTORY_FILE):
        return set()
    with open(HISTORY_FILE, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f)

def append_history(name):
    """ì²˜ë¦¬ëœ í•™êµ ê¸°ë¡ ì¶”ê°€"""
    with open(HISTORY_FILE, "a", encoding="utf-8") as f:
        f.write(f"{name}\n")

def get_google_coordinates(address):
    """êµ¬ê¸€ ë§µìŠ¤ APIë¡œ ì¢Œí‘œ ì¶”ì¶œ"""
    if not address: return {"lat": 35.6812, "lng": 139.7671} # ê¸°ë³¸ê°’ (ë„ì¿„ì—­)
    
    base_url = "https://maps.googleapis.com/maps/api/geocode/json"
    params = {"address": address, "key": GOOGLE_MAPS_API_KEY, "language": "ja"}
    try:
        res = requests.get(base_url, params=params)
        data = res.json()
        if data['status'] == 'OK':
            loc = data['results'][0]['geometry']['location']
            return {"lat": loc['lat'], "lng": loc['lng']}
    except: pass
    return {"lat": 35.6812, "lng": 139.7671}

def clean_json(text):
    """AI ì‘ë‹µì—ì„œ ìˆœìˆ˜ JSON ì¶”ì¶œ"""
    text = text.replace("```json", "").replace("```", "").strip()
    start = text.find("{")
    end = text.rfind("}") + 1
    if start != -1 and end != -1:
        return text[start:end]
    return text

def get_university_info(name_ja, name_en):
    """AIì—ê²Œ ëŒ€í•™ ìƒì„¸ ì •ë³´ ìš”ì²­ (ì˜ë¬¸ ë²„ì „)"""
    print(f"ğŸ« AI Analysis - English: {name_ja}")
    
    prompt = f"""
    You are an expert in Japanese higher education.
    Analyze the university "{name_ja}" ({name_en}) and provide data for a Markdown file in **ENGLISH**.

    [Formatting Rules - IMPORTANT]
    1. **Strict Markdown Tables**: 
       - Ensure a blank line before and after the table.
       - Use `| Header | Header |` format.
       - Separator line MUST be `|---|---|`.
       - Do NOT merge cells or use complex structures.

    [Requirements]
    1. **english_slug**: URL-friendly English name (lowercase, kebab-case). e.g., "waseda-university"
    2. **description_ko**: Write a detailed introduction in **ENGLISH** (Markdown format, 2000+ characters).
       - **MUST use Markdown Tables**: Use tables for 'Faculties list', 'Tuition breakdown', 'Admission stats', etc.
       - Structure:
         - ğŸ« University Overview (History, Reputation)
         - ğŸ“ Faculties & Departments (Use Table)
         - ğŸ’° Tuition & Fees (Use Table: Admission fee, Yearly tuition in JPY)
         - ğŸŒ International Student Support (Dormitory, English programs, Career support)
         - ğŸ“ Campus Location & Access (Use Table for access)
    3. **tuition**: Integer values only (JPY).
    
    [Output Format - JSON Only]
    {{
        "english_slug": "university-name-slug",
        "basic_info": {{
            "name_ja": "{name_ja}",
            "name_en": "{name_en}",
            "address": "Official Japanese Address",
            "website": "Official URL"
        }},
        "stats": {{
            "international_students": "Number (integer)",
            "acceptance_rate": "Rate (string)" 
        }},
        "tuition": {{
            "admission_fee": 200000,
            "yearly_tuition": 1000000
        }},
        "faculties": ["School of Political Science", "School of Law", ...],
        "features": ["SGU", "EJU Required", "English Program", "Dormitory", "Scholarship"],
        "description_ko": "## ğŸ« University Overview\\n\\n(Detailed ENGLISH content with Tables)..."
    }}
    """

    for i in range(3):
        try:
            res = model.generate_content(prompt)
            return json.loads(clean_json(res.text))
        except Exception as e:
            print(f"   âš ï¸ Retry ({i+1}/3)... {e}")
            time.sleep(5)
    return None

def save_to_md(data):
    """MD íŒŒì¼ë¡œ ì €ì¥"""
    # 1. ì¢Œí‘œ êµ¬í•˜ê¸°
    addr = data['basic_info'].get('address')
    coords = get_google_coordinates(addr)
    
    # 2. íŒŒì¼ëª… ê²°ì • (univ_ ì ‘ë‘ì–´ ê°•ì œ)
    raw_slug = data.get('english_slug', data['basic_info']['name_en'].replace(" ", "-").lower())
    if not raw_slug.startswith("univ_"):
        slug = f"univ_{raw_slug}"
    else:
        slug = raw_slug
        
    filename = f"{slug}.md"
    filepath = os.path.join(OUTPUT_DIR, filename)

    # 3. Frontmatter ë°ì´í„° êµ¬ì„±
    frontmatter = {
        "layout": "school",
        "id": slug,
        "title": data['basic_info']['name_ja'],
        "category": "university", # [ì¤‘ìš”] ëŒ€í•™ êµ¬ë¶„ì
        "tags": data.get('features', []),
        "thumbnail": "/static/img/pin-univ.png",
        "location": coords,
        "basic_info": data['basic_info'],
        "stats": data['stats'],
        "tuition": data['tuition'],
        "faculties": data.get('faculties', []),
        "features": data.get('features', [])
    }

    # 4. ë³¸ë¬¸ ë¶„ë¦¬
    description = data.get('description_ko', 'No content available.')

    # 5. íŒŒì¼ ì“°ê¸°
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write("---\n")
        f.write(json.dumps(frontmatter, ensure_ascii=False, indent=2))
        f.write("\n---\n\n")
        f.write(description)
    
    return filename

# ==========================================
# [ë©”ì¸ ì‹¤í–‰]
# ==========================================
def main():
    if not os.path.exists(INPUT_CSV):
        print(f"âŒ {INPUT_CSV} file not found.")
        return

    # ì²˜ë¦¬ëœ ëª©ë¡ ë¡œë“œ
    processed_list = load_history()
    
    # CSV ì½ê¸° ë° í•„í„°ë§
    univ_list = []
    with open(INPUT_CSV, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row['name_ja'] not in processed_list:
                univ_list.append(row)
            
    print(f"ğŸš€ Total Universities: {len(univ_list)} (Already processed: {len(processed_list)})")
    print(f"ğŸ¯ Target for this run: {LIMIT}")

    count = 0
    for univ in tqdm(univ_list):
        if LIMIT > 0 and count >= LIMIT:
            print(f"ğŸ›‘ Limit reached ({LIMIT}). Stopping.")
            break
            
        data = get_university_info(univ['name_ja'], univ['name_en'])
        
        if data:
            filename = save_to_md(data)
            append_history(univ['name_ja']) # ì„±ê³µ ì‹œ ê¸°ë¡
            print(f"   âœ… Saved: {filename}")
            count += 1
            time.sleep(3) # API ì œí•œ ë°©ì§€
        else:
            print(f"   âŒ Failed: {univ['name_ja']}")

if __name__ == "__main__":
    main()