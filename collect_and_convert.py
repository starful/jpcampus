import requests
from bs4 import BeautifulSoup
import json
import time
import random
import re
import os
from tqdm import tqdm
from dotenv import load_dotenv
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted

# ==========================================
# [ì„¤ì •]
# ==========================================
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GOOGLE_MAPS_API_KEY = os.getenv("GOOGLE_MAPS_API_KEY")

if not GEMINI_API_KEY or not GOOGLE_MAPS_API_KEY:
    print("âŒ ì˜¤ë¥˜: .env íŒŒì¼ì— API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

genai.configure(api_key=GEMINI_API_KEY)

MODEL_NAME = 'models/gemini-2.0-flash'
try:
    model = genai.GenerativeModel(MODEL_NAME)
    print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {MODEL_NAME}")
except:
    print("âš ï¸ ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‹œë„")
    model = genai.GenerativeModel('gemini-pro')

OUTPUT_JSON = "file/schools_complete_db.json"

# [ìˆ˜ì •ë¨] ë„ì¿„ + ì¹˜ë°” URL ë¦¬ìŠ¤íŠ¸
TARGET_AREAS = [
    # ë„ì¿„
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E6%9D%B1%E4%BA%AC%E9%83%BD",
    # ì¹˜ë°”
    "https://www.nisshinkyo.org/search/area.php?lng=1&area=%E5%8D%83%E8%91%89"
]

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

# ==========================================
# [1] Google Geocoding (ì¢Œí‘œ ë³€í™˜)
# ==========================================
def get_google_coordinates(address):
    base_url = "https://maps.googleapis.com/maps/api/geocode/json"
    
    # ìš°í¸ë²ˆí˜¸ ë° ê³µë°± ì œê±°, ê±´ë¬¼ëª… ë¶„ë¦¬
    clean_address = re.sub(r'ã€’?\s*\d{3}-\d{4}', '', address).strip()
    if ' ' in clean_address:
        clean_address = clean_address.split(' ')[0]
    
    # [ì¤‘ìš”] ì—¬ê¸°ì— 'æ±äº¬éƒ½' ê°•ì œ ì¶”ê°€ ë¡œì§ ì—†ìŒ (ì¹˜ë°”í˜„ ì£¼ì†Œ ëŒ€ì‘)
    
    params = {"address": clean_address, "key": GOOGLE_MAPS_API_KEY, "language": "ja"}
    
    try:
        res = requests.get(base_url, params=params)
        data = res.json()
        if data['status'] == 'OK':
            loc = data['results'][0]['geometry']['location']
            return {"lat": loc['lat'], "lng": loc['lng']}
    except:
        pass
    # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ (ë„ì¿„ ì‹œì²­)
    return {"lat": 35.6895, "lng": 139.6917}

# ==========================================
# [2] í¬ë¡¤ë§ & AI ë³€í™˜
# ==========================================
def clean_json(text):
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    return text.strip()

def extract_info_ai(school_name, text):
    prompt = f"""
    Extract data to JSON.
    [Rules]
    1. Numbers: Remove commas, convert to Integer.
    2. Missing: Use 0 or null.
    3. Output: ONLY valid JSON string.

    [Schema]
    {{
        "id": "unique_id_english_or_number",
        "basic_info": {{ "name_ja": "School Name", "address": "Address", "capacity": "Capacity(int)" }},
        "student_demographics": {{ "total": "Total(int)", "korea": "Korea(int)", "china": "China(int)", "vietnam": "Vietnam(int)", "nepal": "Nepal(int)", "usa": "USA(int)" }},
        "courses": [ {{ "course_name": "Name", "admission_month": "Month", "total_fees": "1st Year Fee(int)" }} ],
        "career_path": {{ "grad_school": "Grad(int)", "university": "Univ(int)", "vocational": "Vocational(int)" }},
        "features": ["Feature1", "Feature2"]
    }}
    [Text] {school_name} \n {text[:15000]}
    """
    
    max_retries = 3
    for i in range(max_retries):
        try:
            res = model.generate_content(prompt)
            return json.loads(clean_json(res.text))
        except ResourceExhausted:
            wait_time = (i + 1) * 10
            print(f"   â³ API í•œë„ ì´ˆê³¼! {wait_time}ì´ˆ ëŒ€ê¸°...")
            time.sleep(wait_time)
        except Exception as e:
            print(f"   âš ï¸ ë³€í™˜ ì—ëŸ¬: {e}")
            return None
    return None

def get_school_links(target_url):
    try:
        res = requests.get(target_url, headers=HEADERS)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        links = soup.select('a[href*="college.php"]')
        return [{"name": l.get_text(strip=True), "url": f"https://www.nisshinkyo.org/search/{l['href']}"} for l in links]
    except Exception as e:
        print(f"ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨ ({target_url}): {e}")
        return []

def get_page_text(url):
    try:
        time.sleep(random.uniform(0.5, 1.0))
        res = requests.get(url, headers=HEADERS)
        res.encoding = res.apparent_encoding
        if res.status_code != 200: return None
        soup = BeautifulSoup(res.text, 'html.parser')
        for s in soup(["script", "style"]): s.extract()
        return soup.get_text("\n", strip=True)
    except:
        return None

# ==========================================
# [3] ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    if not os.path.exists("file"): os.makedirs("file")
    
    all_schools_meta = []
    
    print("ğŸ” ê° ì§€ì—­ë³„ í•™êµ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘...")
    for area_url in TARGET_AREAS:
        links = get_school_links(area_url)
        print(f"   ã„´ ë°œê²¬: {len(links)}ê°œ í•™êµ")
        all_schools_meta.extend(links)
    
    # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
    seen_urls = set()
    unique_schools = []
    for s in all_schools_meta:
        if s['url'] not in seen_urls:
            unique_schools.append(s)
            seen_urls.add(s['url'])

    final_data = []
    print(f"ğŸš€ ì´ {len(unique_schools)}ê°œ í•™êµ ìƒì„¸ ì •ë³´ ì²˜ë¦¬ ì‹œì‘...")

    for school in tqdm(unique_schools):
        raw_text = get_page_text(school['url'])
        if not raw_text: continue

        data = extract_info_ai(school['name'], raw_text)
        if data:
            data['source_url'] = school['url']
            addr = data['basic_info'].get('address', '')
            if addr:
                data['location'] = get_google_coordinates(addr)
            final_data.append(data)
            time.sleep(2) # API ì†ë„ ì¡°ì ˆ

    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ ì™„ë£Œ! {len(final_data)}ê°œ ì €ì¥ë¨: {OUTPUT_JSON}")

if __name__ == "__main__":
    main()